{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38ca4dc-9f36-41c9-8187-6efbe7b98493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f012047-664d-4255-a45e-2cce6f1542bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_v3a_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3) , input_shape = (48,48,1), activation = 'relu'),\n",
    "    tf.keras.layers.Conv2D(128, (3,3) , activation = 'relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256, (3,3) , activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(512, (3,3) , activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9fece90-b567-44c8-a97e-abb60ba6d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('CNN_v3a_200epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16686443-4a9e-4fbc-841c-22f7381df795",
   "metadata": {},
   "source": [
    "## Loading a test image and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be578bf7-53e1-4c39-af14-71351cf2bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('testimage_cropped.jpg', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8bb73e-ca2f-402a-9562-c9a0b7a1d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('display', test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3dc92da-34da-46af-b33c-8ef6126aaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_resized_gray = cv2.resize(test_image,(48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0335f7de-5745-4704-9abe-8e0c73ae32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('display', test_image_resized_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323caa45-450a-40c6-b6f0-5810541f0f98",
   "metadata": {},
   "source": [
    "## Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce47e5f-70c9-4458-802d-6c65b0340385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(img):\n",
    "    img = cv2.resize(img, (48,48))\n",
    "    prediction = model.predict(img.reshape(1,48,48,1))\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    emotions = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "    \n",
    "    return emotions[int(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd96ed16-eb63-4bfe-b064-1828227e3265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angry'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(test_image_resized_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bedc99-8ecb-4300-84e4-a13742903009",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test/happy/PrivateTest_13103594.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfa48ff-cfa8-4b2c-afd1-123ac4da54f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c245d87-b136-48f6-b7ac-80f00169ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6b77a9-20a8-4996-a21e-963acbf8b10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a75715-add1-4170-99d2-5455f9e780a8",
   "metadata": {},
   "source": [
    "## Using HC to create a bounding box around face found in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8e425f-1942-437a-92ae-872326758320",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_path = 'C:\\\\Users\\\\Randy\\\\Venv\\\\cv\\Lib\\\\site-packages\\\\cv2\\\\data\\\\haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a653a981-fac4-41bb-8e68-4cb2f9f0384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = cv2.CascadeClassifier(hc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4390e9cb-f335-4efe-849e-be923dde2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = hc.detectMultiScale(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b4649a-cf21-4126-b61c-e439d9573d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in faces:\n",
    "    x, y, w, h = face\n",
    "    cv2.rectangle(test_image,(x,y),(x+w, y+h),(0,0,255),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc5d61a-6172-4636-a539-1dd63b324c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(img):\n",
    "    #hc_path = 'C:\\\\Users\\\\Randy\\\\Venv\\\\cv\\Lib\\\\site-packages\\\\cv2\\\\data\\\\haarcascade_frontalface_default.xml'\n",
    "    #hc = cv2.CascadeClassifier(hc_path)\n",
    "    faces = hc.detectMultiScale(img)\n",
    "    for face in faces:\n",
    "        x, y, w, h = face\n",
    "        cv2.rectangle(img,(x,y),(x+w, y+h),(0,0,255),3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d0765e-9be4-4cbf-9ad7-437485fe590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Faces Detected\", test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330b8d08-037c-4788-a643-4ce7d1838d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_path = 'C:\\\\Users\\\\Randy\\\\Venv\\\\cv\\\\Lib\\\\site-packages\\\\cv2\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba81923-55fd-4e64-8e56-6ae8ac852491",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Faces Detected\", draw_bounding_box(test_image))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f783c580-8856-4e82-afb8-b2dff78870e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = faces[0]\n",
    "cropped_face = test_image[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426da9eb-1e39-482f-8458-e6f46e0ba70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cropped_face(img):\n",
    "    faces = hc.detectMultiScale(img)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "257d2e1a-9982-4161-8c77-031eeb453e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Cropped Face\", cropped_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c122ca15-9373-47d2-94d5-1ae4202d5561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surprise'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(cropped_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99eee04e-8639-4ff3-9c57-2f8d00c43cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b44a0a4-af05-4a55-9d2e-3b81ec55a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not video_cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = hc.detectMultiScale(gray)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, \n",
    "            predict_emotion(face), \n",
    "            (50, 50), #top left\n",
    "            font, 1, \n",
    "            (255, 0, 0), #BGR \n",
    "            2, \n",
    "            cv2.LINE_4)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', draw_bounding_box(frame))\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
