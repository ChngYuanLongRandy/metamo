{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38ca4dc-9f36-41c9-8187-6efbe7b98493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25fd84e-6e4a-4918-9fb9-301bd8f72c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96036a8b-765f-4479-94fe-7410f2cf4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [48, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e007537-0d20-49c8-864b-b0c27aebca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(512, (3,3), input_shape=(IMAGE_SHAPE[0],IMAGE_SHAPE[1],1),activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512,(3,3), activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256,(3,3), activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256,(3,3), activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512,activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256,activation = 'relu'),\n",
    "    Dense(7,activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fece90-b567-44c8-a97e-abb60ba6d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('kdef_gray_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16686443-4a9e-4fbc-841c-22f7381df795",
   "metadata": {},
   "source": [
    "## Loading a test image and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be578bf7-53e1-4c39-af14-71351cf2bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('./uploaded_images/testimage.jpg', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8bb73e-ca2f-402a-9562-c9a0b7a1d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('display', test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dc92da-34da-46af-b33c-8ef6126aaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_resized_gray = cv2.resize(test_image,(IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0335f7de-5745-4704-9abe-8e0c73ae32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('display', test_image_resized_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323caa45-450a-40c6-b6f0-5810541f0f98",
   "metadata": {},
   "source": [
    "## Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce47e5f-70c9-4458-802d-6c65b0340385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(img):\n",
    "    img = cv2.resize(img, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))\n",
    "    img = img *(1./255) \n",
    "    prediction = model.predict(img.reshape(1,IMAGE_SHAPE[0],IMAGE_SHAPE[1],1))\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    emotions = ['ANGRY','DISGUSTED','FEARFUL','HAPPY','NEUTRAL','SAD','SURPRISED']\n",
    "\n",
    "    return emotions[int(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd96ed16-eb63-4bfe-b064-1828227e3265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DISGUSTED'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(test_image_resized_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09699da9-5679-4ce4-b9cc-7ed3745dd4ab",
   "metadata": {},
   "source": [
    "## Fearful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bedc99-8ecb-4300-84e4-a13742903009",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fear = cv2.imread('./KDEF/AF01/AF01AFS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_fear = cv2.resize(img_fear, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfa48ff-cfa8-4b2c-afd1-123ac4da54f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_fear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c245d87-b136-48f6-b7ac-80f00169ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_fear)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c6b77a9-20a8-4996-a21e-963acbf8b10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FEARFUL'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_fear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e457501-e32b-4fd3-bffb-7f034b7338a9",
   "metadata": {},
   "source": [
    "## Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e14f0239-1fa4-4b4b-a26a-ed399f58939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_happy = cv2.imread('./KDEF/AF01/AF01HAS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_happy = cv2.resize(img_happy, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab3c057-1c2a-41f8-9449-6fb151749726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_happy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feab9062-a7bb-4df0-8b7e-0ba661685065",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_happy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "415fd046-99c4-4e0a-876d-a9b43058233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAPPY'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eec59e-58a4-44ef-a23a-073fa64c70d1",
   "metadata": {},
   "source": [
    "## Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61eaa485-2765-4dba-b3bd-fa5b2d07d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sad = cv2.imread('./KDEF/AF01/AF01SAS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_sad = cv2.resize(img_sad, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e184ba9a-33ef-4d11-8ba5-93f875edfe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_sad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89434aa3-5149-47a3-beff-ccd388b725d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_sad)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727da24a-5355-4713-a8a4-87588b8647c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAD'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11a052-f919-4006-b1cc-2ef757784ce2",
   "metadata": {},
   "source": [
    "## Disgusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5e5ce7-f52d-4b6e-9594-8781775a4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_disgusted = cv2.imread('./KDEF/AF01/AF01DIS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_disgusted = cv2.resize(img_disgusted, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee5e10e1-a2bd-4f5d-baa4-ae1c93b64c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_disgusted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b293260-e508-464f-8354-3db37452b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_disgusted)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4cc216-3a79-4c1f-989a-0bf90674e9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DISGUSTED'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_disgusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c7dd9-e0ff-48a8-be3f-52d2803475a8",
   "metadata": {},
   "source": [
    "## Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cf834d8-71b6-4c8e-b2e7-101f755e77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sur = cv2.imread('./KDEF/AF01/AF01SUS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_sur = cv2.resize(img_sur, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6477cc89-5eae-43e9-b8ca-1983045c9202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_sur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eee06c8-6f2b-4533-95cc-0c82a6b260b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_sur)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "340738b2-75b7-4b4d-b0c2-5b565590fa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FEARFUL'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_sur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22d343-8be3-4337-82ff-8405791bed00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bc0b52a-0fc2-4af6-9875-9f076805be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_angry = cv2.imread('./KDEF/AF01/AF01ANS.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_angry = cv2.resize(img_angry, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7af1799d-bac6-4edc-9df5-63befea17c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_angry.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1021afdb-2085-4d4e-94fa-86498553f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_angry)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12220e3e-2e2a-4736-9bf9-e39d4519a7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANGRY'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_angry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59ec4e-1607-442d-b0da-0d9acb57cea4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ace0cc-9fc5-4467-80d3-7ed2413bbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_neutral = cv2.imread('./KDEF/AF01/AF01NES.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img_neutral = cv2.resize(img_neutral, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24cddde0-3a76-4af3-a839-fbd48cc3062e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_neutral.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daf0a2c4-3b0e-4bdd-a642-26e39afb5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Test', img_neutral)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2471ad53-1a46-4a5d-b9b0-eddf6a2a3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEUTRAL'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(img_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364dd0a-4ce5-4a41-aff5-4c908738ab79",
   "metadata": {},
   "source": [
    "## Group test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e26c1a42-92ac-49ae-a338-d4d9ae9d0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['ANGRY','DISGUSTED','FEARFUL','HAPPY','NEUTRAL','SAD','SURPRISED']\n",
    "emotions_dict = {'AFS': 'FEARFUL', \n",
    "               'ANS': 'ANGRY',\n",
    "               'DIS': 'DISGUSTED',\n",
    "               'HAS' : 'HAPPY',\n",
    "               'NES': 'NEUTRAL',\n",
    "               'SAS': 'SAD',\n",
    "               'SUS': 'SURPRISED'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bf9e73c-ea86-4823-99b4-e03f4a0ee160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = ['./KDEF/BF17/BF17'+str(key)+'.jpg' for key in emotions_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8438c5d-37dd-499a-bcca-551dfd93bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./KDEF/BF17/BF17AFS.jpg'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1398714-5b5d-4248-8a8c-44034ae346f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))\n",
    "    img = img *(1./255)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5013724a-a711-46d1-9911-ab00c4753700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(path):\n",
    "    img = preprocess_image(path)\n",
    "    prediction = model.predict(img.reshape(1, img.shape[1],img.shape[0],1))\n",
    "    predict_emotion = np.argmax(prediction, axis=1)\n",
    "    predict_emotion = emotions[int(predict_emotion)]\n",
    "    return list(zip(*prediction,emotions)) , predict_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b0e8789-c354-494c-9e64-c1abcd3390a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANGRY', 'DISGUSTED', 'FEARFUL', 'HAPPY', 'NEUTRAL', 'SAD', 'SURPRISED']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a44ad998-bbb8-439f-ab77-86cd77263dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  ./KDEF/BF17/BF17AFS.jpg\n",
      "[(1.1037282e-06, 'ANGRY'), (1.1006997e-06, 'DISGUSTED'), (0.999979, 'FEARFUL'), (4.7358906e-08, 'HAPPY'), (2.1665302e-07, 'NEUTRAL'), (3.453813e-06, 'SAD'), (1.5140823e-05, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17ANS.jpg\n",
      "[(1.0, 'ANGRY'), (3.276869e-10, 'DISGUSTED'), (7.7436946e-11, 'FEARFUL'), (5.8131147e-15, 'HAPPY'), (2.3686905e-11, 'NEUTRAL'), (5.8174084e-11, 'SAD'), (1.7632931e-10, 'SURPRISED')]\n",
      "Predicted emotion is ANGRY\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17DIS.jpg\n",
      "[(5.199602e-12, 'ANGRY'), (1.0, 'DISGUSTED'), (6.163663e-13, 'FEARFUL'), (3.6810743e-13, 'HAPPY'), (2.7032157e-15, 'NEUTRAL'), (2.0592633e-13, 'SAD'), (4.6323038e-15, 'SURPRISED')]\n",
      "Predicted emotion is DISGUSTED\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17HAS.jpg\n",
      "[(1.6758383e-07, 'ANGRY'), (3.559387e-07, 'DISGUSTED'), (0.018429197, 'FEARFUL'), (0.9815637, 'HAPPY'), (2.6229698e-08, 'NEUTRAL'), (6.285659e-06, 'SAD'), (3.0843086e-07, 'SURPRISED')]\n",
      "Predicted emotion is HAPPY\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17NES.jpg\n",
      "[(6.7113892e-06, 'ANGRY'), (7.162608e-08, 'DISGUSTED'), (4.0130093e-05, 'FEARFUL'), (5.132746e-07, 'HAPPY'), (0.99994135, 'NEUTRAL'), (3.1235604e-06, 'SAD'), (8.050796e-06, 'SURPRISED')]\n",
      "Predicted emotion is NEUTRAL\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17SAS.jpg\n",
      "[(5.006891e-11, 'ANGRY'), (3.618997e-09, 'DISGUSTED'), (5.451871e-08, 'FEARFUL'), (3.21896e-14, 'HAPPY'), (1.1244425e-10, 'NEUTRAL'), (1.0, 'SAD'), (2.012304e-14, 'SURPRISED')]\n",
      "Predicted emotion is SAD\n",
      "\n",
      "\n",
      "Testing for  ./KDEF/BF17/BF17SUS.jpg\n",
      "[(1.3413969e-12, 'ANGRY'), (1.6824886e-12, 'DISGUSTED'), (6.858711e-07, 'FEARFUL'), (9.628577e-15, 'HAPPY'), (1.2596932e-12, 'NEUTRAL'), (4.2931392e-13, 'SAD'), (0.9999993, 'SURPRISED')]\n",
      "Predicted emotion is SURPRISED\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for string in test_strings:\n",
    "    print('Testing for ' , string)\n",
    "    print(predict_emotion(string)[0])\n",
    "    print(f'Predicted emotion is {predict_emotion(string)[1]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31a3b3b5-5e16-4727-a7f8-b8f5cc8daf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4498d638-de8d-4cf1-a3cd-96c6f55bcc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_images = os.listdir('./uploaded_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aac6240d-dae1-4fa1-9e0e-fd28f0223067",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_images_paths = ['./uploaded_images/' + path for path in uploaded_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9ebaec4-2894-44ee-a3f8-3ca63b415bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./uploaded_images/20200202_165122.jpg',\n",
       " './uploaded_images/20210815_140224.jpg',\n",
       " './uploaded_images/DSC00003 2007.JPG',\n",
       " './uploaded_images/DSC00732.JPG',\n",
       " './uploaded_images/Image011.jpg',\n",
       " './uploaded_images/IMG-20140402-WA0006.jpg',\n",
       " './uploaded_images/IMG-20170705-WA0002.jpeg',\n",
       " './uploaded_images/IMG-20170705-WA0008.jpeg',\n",
       " './uploaded_images/testimage.jpg',\n",
       " './uploaded_images/testimage_cropped.jpg']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded_images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2241e2d9-d642-40ac-8b53-f7e8e7b1f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  ./uploaded_images/20200202_165122.jpg\n",
      "[(0.013627761, 'ANGRY'), (0.9703776, 'DISGUSTED'), (0.0001119458, 'FEARFUL'), (0.00023146962, 'HAPPY'), (0.00046147074, 'NEUTRAL'), (0.01518865, 'SAD'), (9.677409e-07, 'SURPRISED')]\n",
      "Predicted emotion is DISGUSTED\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/20210815_140224.jpg\n",
      "[(0.00040714245, 'ANGRY'), (0.85497946, 'DISGUSTED'), (0.1435458, 'FEARFUL'), (1.2694131e-05, 'HAPPY'), (1.3145025e-07, 'NEUTRAL'), (0.0010544043, 'SAD'), (3.9031116e-07, 'SURPRISED')]\n",
      "Predicted emotion is DISGUSTED\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/DSC00003 2007.JPG\n",
      "[(0.00076980115, 'ANGRY'), (0.022152817, 'DISGUSTED'), (0.0014536582, 'FEARFUL'), (5.2461466e-05, 'HAPPY'), (0.0002781727, 'NEUTRAL'), (0.9752925, 'SAD'), (6.3211667e-07, 'SURPRISED')]\n",
      "Predicted emotion is SAD\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/DSC00732.JPG\n",
      "[(0.9375957, 'ANGRY'), (0.061139535, 'DISGUSTED'), (0.0010047571, 'FEARFUL'), (0.00016670146, 'HAPPY'), (3.059989e-06, 'NEUTRAL'), (7.204361e-05, 'SAD'), (1.8248164e-05, 'SURPRISED')]\n",
      "Predicted emotion is ANGRY\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/Image011.jpg\n",
      "[(0.04191797, 'ANGRY'), (0.014265524, 'DISGUSTED'), (0.91107607, 'FEARFUL'), (0.00016625006, 'HAPPY'), (2.7811708e-08, 'NEUTRAL'), (0.03257408, 'SAD'), (4.0337698e-09, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/IMG-20140402-WA0006.jpg\n",
      "[(0.079182476, 'ANGRY'), (0.09494999, 'DISGUSTED'), (0.69066393, 'FEARFUL'), (0.00054134266, 'HAPPY'), (0.00078033336, 'NEUTRAL'), (0.13333082, 'SAD'), (0.00055112626, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/IMG-20170705-WA0002.jpeg\n",
      "[(1.25578235e-05, 'ANGRY'), (4.8775835e-08, 'DISGUSTED'), (0.99955577, 'FEARFUL'), (2.7588663e-12, 'HAPPY'), (2.3353167e-13, 'NEUTRAL'), (0.0004315987, 'SAD'), (1.6915873e-15, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/IMG-20170705-WA0008.jpeg\n",
      "[(0.11983995, 'ANGRY'), (0.16153479, 'DISGUSTED'), (0.6964245, 'FEARFUL'), (0.00010273461, 'HAPPY'), (0.00015321943, 'NEUTRAL'), (0.021849565, 'SAD'), (9.530947e-05, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/testimage.jpg\n",
      "[(0.00040714245, 'ANGRY'), (0.85497946, 'DISGUSTED'), (0.1435458, 'FEARFUL'), (1.2694131e-05, 'HAPPY'), (1.3145025e-07, 'NEUTRAL'), (0.0010544043, 'SAD'), (3.9031116e-07, 'SURPRISED')]\n",
      "Predicted emotion is DISGUSTED\n",
      "\n",
      "\n",
      "Testing for  ./uploaded_images/testimage_cropped.jpg\n",
      "[(0.0036194297, 'ANGRY'), (0.0015540763, 'DISGUSTED'), (0.9895698, 'FEARFUL'), (8.577028e-06, 'HAPPY'), (8.3036364e-05, 'NEUTRAL'), (0.001323064, 'SAD'), (0.0038420092, 'SURPRISED')]\n",
      "Predicted emotion is FEARFUL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for string in uploaded_images_paths:\n",
    "    print('Testing for ' , string)\n",
    "    print(predict_emotion(string)[0])\n",
    "    print(f'Predicted emotion is {predict_emotion(string)[1]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a75715-add1-4170-99d2-5455f9e780a8",
   "metadata": {},
   "source": [
    "## Using HC to create a bounding box around face found in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f8e425f-1942-437a-92ae-872326758320",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_path = '.\\haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a653a981-fac4-41bb-8e68-4cb2f9f0384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = cv2.CascadeClassifier(hc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924f277-ad6a-49de-868c-b777b6e444dc",
   "metadata": {},
   "source": [
    "for face in faces:\n",
    "    x, y, w, h = face\n",
    "    cv2.rectangle(test_image,(x - adjust_param[0],y - adjust_param[1]),(x+w +adjust_param[0], y+h + adjust_param[1]),(0,0,255),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455fc0-615a-470f-8884-bd83a47ed803",
   "metadata": {},
   "source": [
    "Test from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7bed4-9005-4d99-86ce-37a8b22ed148",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_param = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5d61a-6172-4636-a539-1dd63b324c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(img):\n",
    "    faces = hc.detectMultiScale(img)\n",
    "    for face in faces:\n",
    "        x, y, w, h = face\n",
    "        cv2.rectangle(img,(x - adjust_param[0],y - adjust_param[1]),(x+w +adjust_param[0], y+h + adjust_param[1]),(0,0,255),3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867935e3-8642-4b69-be42-d40e250f9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('./uploaded_images/IMG-20140402-WA0006.jpg', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba81923-55fd-4e64-8e56-6ae8ac852491",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Faces Detected\", draw_bounding_box(test_image))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43c0ed-447a-4b44-b18f-2f0700cf777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_param = [100,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358e665-f9a2-4ffe-99a5-7535ffb3d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box_changes(img):\n",
    "    faces = hc.detectMultiScale(img)\n",
    "    for face in faces:\n",
    "        x, y, w, h = face\n",
    "        cv2.rectangle(img,(x - adjust_param[0],y - adjust_param[1]),(x+w +adjust_param[0], y+h + adjust_param[1]),(0,255,0),3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4a9ef-7322-4f9f-beb6-f7c15c652903",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('./uploaded_images/IMG-20140402-WA0006.jpg', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0765e-9be4-4cbf-9ad7-437485fe590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Faces Detected\", draw_bounding_box_changes(test_image))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f42bd-1407-422c-8cff-38e7fe7eec97",
   "metadata": {},
   "source": [
    "Test ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "741dbe32-661c-4e5c-bd52-633dfb123e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Test Image\", test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4390e9cb-f335-4efe-849e-be923dde2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = hc.detectMultiScale(test_image)#,minNeighbors = 10,scaleFactor = 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f783c580-8856-4e82-afb8-b2dff78870e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adjust_param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m x, y, w, h \u001b[38;5;241m=\u001b[39m faces[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m cropped_face \u001b[38;5;241m=\u001b[39m test_image[y\u001b[38;5;241m-\u001b[39m \u001b[43madjust_param\u001b[49m[\u001b[38;5;241m1\u001b[39m]:y\u001b[38;5;241m+\u001b[39mh\u001b[38;5;241m+\u001b[39m adjust_param[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m-\u001b[39m adjust_param[\u001b[38;5;241m0\u001b[39m]:x\u001b[38;5;241m+\u001b[39mw\u001b[38;5;241m+\u001b[39madjust_param[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adjust_param' is not defined"
     ]
    }
   ],
   "source": [
    "x, y, w, h = faces[0]\n",
    "cropped_face = test_image[y- adjust_param[1]:y+h+ adjust_param[1], x- adjust_param[0]:x+w+adjust_param[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d2e1a-9982-4161-8c77-031eeb453e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Cropped Face\", cropped_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "426da9eb-1e39-482f-8458-e6f46e0ba70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cropped_face(img):\n",
    "    faces = hc.detectMultiScale(img)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "172095d4-088f-4bfd-9783-ce61cb341aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(img):\n",
    "    img = cv2.resize(img, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))\n",
    "    img = img *(1./255) \n",
    "    prediction = model.predict(img.reshape(1,IMAGE_SHAPE[0],IMAGE_SHAPE[1],1))\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    emotions = ['ANGRY','DISGUSTED','FEARFUL','HAPPY','NEUTRAL','SAD','SURPRISED']\n",
    "\n",
    "    return emotions[int(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c122ca15-9373-47d2-94d5-1ae4202d5561",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cropped_face' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predict_emotion(\u001b[43mcropped_face\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cropped_face' is not defined"
     ]
    }
   ],
   "source": [
    "predict_emotion(cropped_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37f4e778-166e-4e8d-ad5a-7e4da00ca724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_vidcap(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (IMAGE_SHAPE[0],IMAGE_SHAPE[1]))\n",
    "    img = img *(1./255)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6fb418a-fcd0-417a-98f7-83ab6a056147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_vidcap(img):\n",
    "    img = preprocess_image_vidcap(img)\n",
    "    prediction = model.predict(img.reshape(1, img.shape[1],img.shape[0],1))\n",
    "    predict_emotion = np.argmax(prediction, axis=1)\n",
    "    predict_emotion = emotions[int(predict_emotion)]\n",
    "    return predict_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99eee04e-8639-4ff3-9c57-2f8d00c43cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44a0a4-af05-4a55-9d2e-3b81ec55a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not video_cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    # Our operations on the frame come here\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #faces = hc.detectMultiScale(gray)\n",
    "    \n",
    "    # for (x,y,w,h) in faces:\n",
    "    #     face = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, \n",
    "            predict_emotion_vidcap(frame), \n",
    "            (50, 50), #top left\n",
    "            font, 1, \n",
    "            (255, 0, 0), #BGR \n",
    "            2, \n",
    "            cv2.LINE_4)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
